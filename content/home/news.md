+++
# Selected Talks widget.
widget = "talks_selected"
active = true 
date = 2016-04-20T00:00:00

title = "News"
subtitle = ""

# Order that this section will appear in.
weight = 10

# List format.
#   0 = Simple
#   1 = Detailed
list_format = 0
+++
- (2026.1) üî• The [Falcon-H1-Tiny Series](https://huggingface.co/collections/tiiuae/falcon-h1-tiny) is released! A new family of extremely compact yet remarkably powerful language models: 90M English, 90M Coder, 90M Tool/Function Calling, 100M Multilingual, and 90M / 0.6B Reasoning. More insights can be found in our [blogpost](https://huggingface.co/spaces/tiiuae/tiny-h1-blogpost)
- (2026.1) The [Learnable Multiplier](https://arxiv.org/abs/2601.04890) paper is out, extending the Œº-parameterization used in [Falcon-H1](https://huggingface.co/papers/2507.22448) with learnable scaling for improved LLM performance, successfully applied in [Falcon-H1-Tiny](https://huggingface.co/collections/tiiuae/falcon-h1-tiny).
- (2025.7) The [technical report](https://arxiv.org/abs/2507.22448) of Falcon-H1 is released.
- (2025.6) The [E2LM competition](https://e2lmc.github.io/) got accepted by [NeurIPS'25](https://neurips.cc/), focused on early-stage training evaluations of LLMs. Registration is now open üöÄ The first prize is 6,000 USDüî•
- (2025.5) üî• The [Falcon-H1 Series](https://huggingface.co/collections/tiiuae/falcon-h1-6819f2795bc406da60fab8df) is released! Featuring a novel hybrid Transformer‚ÄìSSM architecture, Falcon-H1 sets a new state of the art (SoTA) across every scale‚Äî0.5B, 1.5B, 3B, 7B, and 34B, and outperforms leading Transformer models double its size. [Blogpost](https://falcon-lm.github.io/blog/falcon-h1/) üîç 
- (2025.5) üöÄ We just released [Falcon Edge](https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130)! A series of powerful, universal, fine-tunable 1.58bit language models, check the blogpost [here](https://falcon-lm.github.io/blog/falcon-edge/).
- (2024.12) [Falcon 3](https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026) is out! Five base models + their Instruct versions: 1B, 3B, 7B, 10B and a more advanced 7B Mamba model, check the blogpost [here](https://huggingface.co/blog/falcon3).
- (2024.10) The technical report for Falcon Mamba 7B is released - check it [here](https://arxiv.org/pdf/2410.05355).
- (2024.08) Falcon Mamba 7B is released - the first strong attention-free 7B language model. Try it out in [HuggingFace](https://huggingface.co/tiiuae/falcon-mamba-7b), check the blogpost [here](https://huggingface.co/blog/falconmamba).
