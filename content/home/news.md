+++
# Selected Talks widget.
widget = "talks_selected"
active = true 
date = 2016-04-20T00:00:00

title = "News"
subtitle = ""

# Order that this section will appear in.
weight = 10

# List format.
#   0 = Simple
#   1 = Detailed
list_format = 0
+++
- (2025.5) üî• We‚Äôre thrilled to announce the release of [Falcon-H1 Series](https://huggingface.co/collections/tiiuae/falcon-h1-6819f2795bc406da60fab8df)! Featuring a novel hybrid Transformer‚ÄìSSM architecture, Falcon-H1 sets a new state of the art (SoTA) across every scale‚Äî0.5B, 1.5B, 3B, 7B, and 34B, and outperforms leading Transformer models double its size. 
Dive into the details in our latest [blogpost](https://falcon-lm.github.io/blog/falcon-h1/) üîç 
- (2025.5) üöÄ We just released [Falcon Edge](https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130)! A series of powerful, universal, fine-tunable 1.58bit language models, check the blogpost [here](https://falcon-lm.github.io/blog/falcon-edge/).
- (2024.12) [Falcon 3](https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026) is out! Five base models + their Instruct versions: 1B, 3B, 7B, 10B and a more advanced 7B Mamba model, check the blogpost [here](https://huggingface.co/blog/falcon3).
- (2024.10) The technical report for Falcon Mamba 7B is released - check it [here](https://arxiv.org/pdf/2410.05355).
- (2024.08) Falcon Mamba 7B is released - the first strong attention-free 7B language model. Try it out in [HuggingFace](https://huggingface.co/tiiuae/falcon-mamba-7b), check the blogpost [here](https://huggingface.co/blog/falconmamba).
